\documentclass[17pt, unicode,dvipdfmx]{beamer}
\usetheme{boxes}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{listings}
\usepackage{enumerate}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

\usepackage{multirow}
\usepackage {booktabs}

\makeatletter
\setbeamertemplate{headline}{%
    \begin{beamercolorbox}[ht=2.25ex,dp=3.75ex]{section in head/foot}
        \insertnavigation{\paperwidth}
    \end{beamercolorbox}%
}
\makeatother
 

\title{Paper Reading}
\subtitle{ A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss}

\author{hukuda222}
\date{2020/1/15}

\begin{document}

\frame[plain]{\maketitle}

\begin{frame}{}
 \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{Introduction}
    \begin{description}
      \item [Task]~\\ Generate summary from article (CNN/daily mail).
      \item [Contribution] ~\\They combine the extractive model and the abstractive model for summarization.\\
        They also introduced new loss functions.
  \end{description}
\end{frame}

\section{Models}
\begin{frame}{Hierachical RNN}
\includegraphics[width=10cm]{rinko1.png}
\end{frame}

\begin{frame}{Hierachical Attention}
  \small
  $\alpha_m^t$:the weight of the $t$-th word in $m$-th sentence.\\
  $\beta_n$:the propability of the $n$-th sentence been extracted into the summary.\\
  $n(m)$:the position of the sentence with $m$-th word.
  \normalsize
  \begin{align}
    \hat{\alpha}_m^t=\frac{\alpha^t_m\times \beta_{n(m)}}{\sum_m\alpha^t_m\times \beta_{n(m)}}
  \end{align}
\end{frame}


\begin{frame}{Inconsistency Loss}
  \small
  \begin{align}
    L_{inc}=-\frac{1}{T}\sum_{t=1}^T log(\frac{1}{|\kappa|} \sum_{m\in \kappa}\alpha^t_m\times \beta_{n(m)})
  \end{align}
  $\kappa$ is the set of top K attended words and\\ $T$ is the number of words in the summary.\\~\\
\includegraphics[width=10cm]{rinko2.png}
\end{frame}

\begin{frame}{Extractive Model}
  \small
  Their extractor is inspired by Nallapati et al. (2017).\\
  $\beta_n$:the propability of the $n$-th sentence been extracted into the summary.\\
  $g_n$:the ground-truth label for the $n$-th sentence.\\$g_n\in \{0,1\}$
  \begin{align}
    L_{ext}=-\frac{1}{N}\sum_{n=1}^N log(g_n log \beta_n+(1-g_n)log(1-\beta_n))
  \end{align}
\end{frame}


\begin{frame}{Abstractive Model}
\includegraphics[width=11cm]{rinko3.png}
\end{frame}

\begin{frame}{Abstractive Model}
  \small
  \begin{align}
    &P_w^{final}(\hat{\alpha}^t)=p_{gen}(h^*(\hat{\alpha}^t))P_w^{vocab}(\hat{\alpha}^t)\notag \\
    &~~~~+(1-p_{gen}(h*(\hat{\alpha}^t)))\sum_{m:w_m=w}\hat{\alpha}_m^t\\
    &P^{vocab}(h^*(\hat{\alpha}^t))\notag &\\
    &~~~~=softmax(W_2(W_1[h_t^d,h^*(\hat{\alpha}^t)]+b_1)+b_2)\\
    &h^*(\hat{\alpha}^t)=\sum_m^M \hat{\alpha}_m^t\times h_m^e
  \end{align}
  The model can output OOV word.
\end{frame}


\begin{frame}{Abstractive Model}
  \small
  $\hat{y}^t$:the $t$-th token in the target summary.
  \begin{align}
    L_{abs}&=\frac{1}{T} \sum_{t=1}^T log P_{\hat{y}^t}^{final}(\hat{\alpha}^t)\\
    L_{cov}&=\frac{1}{T} \sum_{t=1}^T \sum_{m=1}^M min(\hat{\alpha}^t,c_m^t)\\
    c_m^t &= \sum_{t'=0}^{t-1}\hat{\alpha}^t_m
  \end{align}
  $L_{cov}$ prevents repetition of the same word.
\end{frame}

\section{Training Procedures}
\begin{frame}{Two-stages training}
  \begin{enumerate}
    \item train the extractive model.
    \item train the abstractive model using the output from the extractive model as input.
  \end{enumerate}
\end{frame}


\begin{frame}{End-to-end training}
  It is a method to train two models at the same time.\\
  They use a weighted sum of the four error functions as the error function.
  \begin{align}
    L_{e2e}=\lambda_1 L_{ext}+\lambda_2 L_{abs}+\lambda_3 L_{cov} + \lambda_4 L_{inc}
  \end{align}
\end{frame}

\section{Exprements}
\begin{frame}{Setting}
    \begin{description}
      \item [Dataset]~\\ Non-anonymized CNN/Daily Mail.
      \item [Pre-training for two-stages training] ~\\They pre-train two models.\\The maximum number of sentence and
        token is 50\\
        They limit the length of the source text to 400 and the length of the summary to 100.
  \end{description}
\end{frame}


\begin{frame}{Setting}
    \begin{description}
      \item [Two-stages training]~\\ The abstracter takes extracted sentences with $\beta>0.5$, where $\beta$ is obtained from the pre-trained extractor.
    \end{description}
\end{frame}

\begin{frame}{Setting}
    \begin{description}
          \item [End-to-end training] ~\\ they will minimize four loss functions with
$\lambda_1 = 5$ and $\lambda_2 = \lambda_3 = \lambda_4 = 1$. \\
        They set $K$ to $3$ for computing $L_{inc}$\\
        They increase the maximum length of source text to $600$.\\
    \end{description}
\end{frame}


\begin{frame}{Setting}
    \begin{description}
      \item [Detail]~\\ 
        \begin{itemize}
          \item Word embeddings size:$128$
          \item Vocabulary size:$50$k
          \item Hidden dimension:\\~~~~$200$(extractor),$256$(abstractor).
          \item Summary length:\\~~~~$120$ in the testing phase.
        \end{itemize}
    \end{description}
\end{frame}

\section{Results}
\begin{frame}{Results of Extracted Sentences}
\includegraphics[width=11cm]{rinko4.png}
ROUGE recall has improved a lot.
\end{frame}

\begin{frame}{Results of Abstractive Sentences}
  \includegraphics[width=11cm]{rinko5.png}
The model of the highest proposed method was SOTA at that time.
\end{frame}

\begin{frame}{Results of Abstractive Sentences}
  \includegraphics[width=11cm]{rinko6.png}
Their model scored higher than the reference in informativity and readability.
\end{frame}

\begin{frame}{The effect of Inconsistency Loss}
   $R_{inc}$ is the probability that the sentence with maximum weighted word have low attention.(i.e., $\beta_{n(argmax(\alpha^t))}<\bar{\beta}$ in each decoder step $t$.
  \begin{align}
    R_{inc}=\frac{Count(t_{inc})}{T}
  \end{align}
  \includegraphics[width=7cm]{rinko7.png}
\end{frame}

\begin{frame}{Output}
  \includegraphics[width=11cm]{rinko8.png}
\end{frame}

\section{Conclusion}
\begin{frame}{Conclusion}
  \begin{itemize}
    \item  They propose a unified model combining the
strength of extractive and abstractive summarization. \\
 \item The inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions.\\
 \item The loss enables extractive and abstractive summarization to be mutually beneficial. 
  \end{itemize}
  \end{frame}

\end{document}

